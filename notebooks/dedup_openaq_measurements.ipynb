{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40359354",
   "metadata": {},
   "source": [
    "# Deduplicate temperature measurements in silver layer\n",
    "- Source table: *bronze openaq_measurements*\n",
    "- Target table: *silver openaq_measurements*\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f8ba3",
   "metadata": {},
   "source": [
    "### Imports and common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0af300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, concat, sha2, split # type: ignore\n",
    "from delta.tables import * # type: ignore\n",
    "\n",
    "catalog = \"air_polution_analytics_dev\"\n",
    "landing_schema = \"00_landing\"\n",
    "bronze_schema = \"01_bronze\"\n",
    "silver_schema = \"02_silver\"\n",
    "source_table = \"openaq_measurements\"\n",
    "target_table = \"openaq_measurements\"\n",
    "\n",
    "base_path = f\"/Volumes/{catalog}/{silver_schema}/metadata\"\n",
    "openaq_path = f\"{base_path}/openaq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0110e0",
   "metadata": {},
   "source": [
    "### Create deduped table, volume and directory for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table = f\"create table if not exists {catalog}.{silver_schema}.{target_table} (id STRING, datetime_from TIMESTAMP, value FLOAT, location_id INT, sensor_id INT);\"\n",
    "create_volume = f\"create volume if not exists {catalog}.{silver_schema}.metadata;\"\n",
    "\n",
    "spark.sql(create_table)\n",
    "spark.sql(create_volume)\n",
    "dbutils.fs.mkdirs(openaq_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec812c8",
   "metadata": {},
   "source": [
    "### Read streaming table, set up a write stream and a deduplication function\n",
    "Generate a unique id for every record in the stream and use it as deduplication key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d35e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "    tableDeduped = DeltaTable.forName(spark, f\"{catalog}.{silver_schema}.{target_table}\")\n",
    "    (tableDeduped.alias(\"t\").merge(\n",
    "        microBatchOutputDF.alias(\"s\"),\n",
    "        \"s.id = t.id\")\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    "    )\n",
    "    \n",
    "df = (spark.readStream\n",
    "    .table(f\"{catalog}.{bronze_schema}.{source_table}\")\n",
    "    .withColumn(\"id\", sha2(concat(col(\"sensor_id\"), col(\"location_id\"), col(\"datetime_from\")), 256))\n",
    ")\n",
    "\n",
    "(df.writeStream\n",
    "  .foreachBatch(upsertToDelta)\n",
    "  .outputMode(\"update\")\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", f\"{openaq_path}/_schema\")\n",
    "  .start()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
