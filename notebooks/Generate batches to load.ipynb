{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e66339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "catalog = \"air_polution_analytics_dev\"\n",
    "bronze_schema = \"01_bronze\"\n",
    "landing_schema = \"00_landing\"\n",
    "files_to_process = 100\n",
    "\n",
    "landing_dir = f\"/Volumes/{catalog}/{landing_schema}/openaq/measurements\"\n",
    "\n",
    "skip_files = []\n",
    "if spark.catalog.tableExists(f\"{catalog}.{bronze_schema}.air_quality_measurements\"):\n",
    "    df = spark.read.table(f\"{catalog}.{bronze_schema}.air_quality_measurements\")\n",
    "    skip_files = [str(f.source_file_name) for f in df.select(\"source_file_name\").distinct().collect()]\n",
    "\n",
    "print(f'found {len(skip_files)} files to skip')\n",
    "\n",
    "datastore_path = dbutils.widgets.get(\"datastore_path\")\n",
    "files = [f.split('/').pop().replace('.zip', '') for f in glob.glob(f\"{datastore_path}/[0-9]*.json.zip\")]\n",
    "print(f'found {len(files)} files')\n",
    "\n",
    "files = list(filter(lambda f: f not in skip_files, files))\n",
    "files.sort()    # To guarantee the order of ingested records\n",
    "\n",
    "print(f'found {len(files)} files to ingest')\n",
    "\n",
    "files = [f\"{datastore_path}/{f}.zip\" for f in files]\n",
    "\n",
    "to_delete = glob.glob(f\"{landing_dir}/*.json\")\n",
    "if to_delete:\n",
    "    subprocess.run([\"rm\"] + to_delete)\n",
    "\n",
    "for f in files[0:files_to_process]:\n",
    "    subprocess.run(f\"unzip -o -d{landing_dir} {f}\".split(\" \"), stdout=subprocess.DEVNULL)\n",
    "\n",
    "print(f\"ingesting {len(files[0:files_to_process])} files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
